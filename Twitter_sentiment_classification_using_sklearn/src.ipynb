{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import string\n",
    "import shutil\n",
    "from operator import add\n",
    "from __future__ import print_function\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "listOfAllTrainingTweets = []\n",
    "stopWords = stopwords.words('english')\n",
    "snowBallStemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanTweets(tweetText):\n",
    "    cleanedTokens = []\n",
    "    listOfAllTrainingTweets.append(tweetText)\n",
    "    tweetText = tweetText.strip('\\n ')\n",
    "    tweetText = tweetText.lower().replace('\\n',' ')\n",
    "    tweetText = re.sub(r'https?:\\/\\/.[^\\s]+', 'URL', tweetText)\n",
    "    tweetText = re.sub(r'www\\.[^\\s]+', 'URL', tweetText)\n",
    "    tweetText = re.sub(r'<[A-Za-z]*>', '', tweetText)\n",
    "    tweetText = re.sub(r'<\\/[A-Za-z]*>', '', tweetText)\n",
    "    tweetText = re.sub(r'@[^\\s]+', 'AT_USER', tweetText)\n",
    "    tweetText = re.sub(r'AT_USER', '', tweetText)\n",
    "    tweetText = re.sub(r'URL', '', tweetText)\n",
    "    tweetText = re.sub(r'\\\"?b[^A-Za-z]', '', tweetText)\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tweetText = regex.sub('', tweetText)\n",
    "    tweetText = re.sub(r'(.)\\1{2,}', r'\\1\\1', tweetText)\n",
    "    tokens = tweetText.split()\n",
    "    for token in tokens:\n",
    "        token = token.strip('\"')\n",
    "        cleanedTokens.append(snowBallStemmer.stem(token))\n",
    "    #cleanedTokens = list(set(tweetText.split()))\n",
    "    cleanedTokens = tweetText.split()\n",
    "    for stopWord in stopWords:\n",
    "        if stopWord in cleanedTokens:\n",
    "            cleanedTokens.remove(stopWord)\n",
    "    #print(cleanedTokens)\n",
    "    return cleanedTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:4: DeprecationWarning: 'U' mode is deprecated\n"
     ]
    }
   ],
   "source": [
    "# acro_dic dictionary\n",
    "acro_dic = {}\n",
    "# Open the file in Universal mode\n",
    "with open('/home/hadoop/data/acronyms.csv', 'rU') as f:\n",
    "    # Get the CSV reader and skip header\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        # First column is the key, the rest is value\n",
    "        acro_dic[row[0]] = row[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanTweets1(tweetText):\n",
    "    cleanedTokens = []\n",
    "    listOfAllTrainingTweets.append(tweetText)\n",
    "    tweetText = tweetText.strip('\\n ')\n",
    "    tweetText = tweetText.lower().replace('\\n',' ')\n",
    "    tweetText = re.sub(r'https?:\\/\\/.[^\\s]+', 'URL', tweetText)\n",
    "    tweetText = re.sub(r'www\\.[^\\s]+', 'URL', tweetText)\n",
    "    tweetText = re.sub(r'<[A-Za-z]*>', '', tweetText)\n",
    "    tweetText = re.sub(r'<\\/[A-Za-z]*>', '', tweetText)\n",
    "    tweetText = re.sub(r'@[^\\s]+', 'AT_USER', tweetText)\n",
    "    tweetText = re.sub(r'AT_USER', '', tweetText)\n",
    "    tweetText = re.sub(r'URL', '', tweetText)\n",
    "    tweetText = re.sub(r'\\\"?b[^A-Za-z]', '', tweetText)\n",
    "    tweetText = re.sub(r'(.)\\1{2,}', r'\\1\\1', tweetText)\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens = tweetText.split()\n",
    "    for token in tokens:\n",
    "        if token in acro_dic:\n",
    "            tokens.extend(acro_dic[token][0].lower().split())\n",
    "            tokens.remove(token)\n",
    "            continue\n",
    "        token = regex.sub('', token)\n",
    "        cleanedTokens.append(PorterStemmer().stem(token))\n",
    "    for token in cleanedTokens:\n",
    "        if token in stopWords:\n",
    "            cleanedTokens.remove(token)\n",
    "    return cleanedTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseForSkLearn(filename):\n",
    "    features = []\n",
    "    labels = []\n",
    "    vocabulary = []\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        for row in csvfile:\n",
    "            features.append(' '. join(cleanTweets1(row.split(',', 1)[1])))\n",
    "            vocabulary.extend(cleanTweets1(row.split(',', 1)[1]))\n",
    "            labels.append(float(row.split(',', 1)[0]))\n",
    "    return [labels, features, vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelsObama, featuresObama, vocabObama = parseForSkLearn('/home/hadoop/data/obama.csv')\n",
    "labelsRomney, featuresRomney, vocabRomney = parseForSkLearn('/home/hadoop/data/romney.csv')\n",
    "\n",
    "labelsObamaTest, featuresObamaTest, vocabObamaTest = parseForSkLearn('/home/hadoop/data/obama_test.csv')\n",
    "labelsRomneyTest, featuresRomneyTest, vocabRomneyTest = parseForSkLearn('/home/hadoop/data/romney_test.csv')\n",
    "#print(featuresObama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#   print(featuresObama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "def mesaureModelPerformanceSklearn(label, pred):\n",
    "    precision, recall, fscore, support = metrics.precision_recall_fscore_support(label, pred)\n",
    "    accuracy = metrics.accuracy_score(label, pred)\n",
    "    return [precision[0], precision[2], recall[0], recall[2], fscore[0], fscore[2], accuracy]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def performKFoldTestingTFIDF(vocabulary, features, labels, modelName, model, dataName):\n",
    "    totalNPrecision = 0\n",
    "    totalPPrecision = 0\n",
    "    totalNRecall = 0\n",
    "    totalPRecall = 0\n",
    "    totalNFscore = 0\n",
    "    totalPFscore = 0\n",
    "    totalAccuracy = 0\n",
    "    y = np.array(labels)\n",
    "    count_vectorizer = CountVectorizer(min_df=1.0,vocabulary=set(vocabulary),ngram_range=(1, 3))\n",
    "    count_vectorizer.fit(features)\n",
    "    tfidf = TfidfTransformer(norm=\"l2\")\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        X_train, X_test, y_train, y_test = train_test_split( features, y, test_size=0.10, random_state=42)\n",
    "        print(\"Without over sampling X_train length=\" + str(len(X_train)))\n",
    "        freq_term_matrix = count_vectorizer.transform(X_train)\n",
    "        #print(freq_term_matrix.shape)\n",
    "        tfidf.fit(freq_term_matrix)\n",
    "        tf_idf_matrix = tfidf.transform(freq_term_matrix)\n",
    "        freq_term_matrix_test = count_vectorizer.transform(X_test)\n",
    "        tfidf.fit(freq_term_matrix_test)\n",
    "        tf_idf_matrix_test = tfidf.transform(freq_term_matrix_test)\n",
    "        model.fit(tf_idf_matrix.toarray(), y_train)\n",
    "        y_pred = model.predict(tf_idf_matrix_test.toarray())\n",
    "        nprecision, pprecision, nrecall, precall, nfscore, pfscore, accuracy = mesaureModelPerformanceSklearn(y_test, y_pred)\n",
    "        totalNPrecision += nprecision\n",
    "        totalPPrecision += pprecision\n",
    "        totalNRecall += nrecall\n",
    "        totalPRecall += precall\n",
    "        totalNFscore += nfscore\n",
    "        totalPFscore += pfscore\n",
    "        totalAccuracy += accuracy\n",
    "    print(dataName + \" data metrics\")\n",
    "    print (modelName + \" classifier avg positive class precision:\" + str((totalPPrecision/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class recall:\" + str((totalPRecall/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class fscore:\" + str((totalPFscore/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class precision:\" + str((totalNPrecision/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class recall:\" + str((totalNRecall/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class fscore:\" + str((totalNFscore/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg accuracy:\" + str((totalAccuracy/10) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "def performKFoldWithOverSamplingTFIDF(vocabulary, features, labels, modelName, model, dataName):\n",
    "    #print (\"features=\" + str(features))\n",
    "    #print (\"labels=\" + str(labels))\n",
    "    totalNPrecision = 0\n",
    "    totalPPrecision = 0\n",
    "    totalNRecall = 0\n",
    "    totalPRecall = 0\n",
    "    totalNFscore = 0\n",
    "    totalPFscore = 0\n",
    "    totalAccuracy = 0\n",
    "    y = np.array(labels)\n",
    "    count_vectorizer = CountVectorizer(min_df=1, vocabulary=set(vocabulary), ngram_range=(1, 3))\n",
    "    count_vectorizer.fit(features)\n",
    "    tfidf = TfidfTransformer(norm=\"l2\")\n",
    "    ros = RandomOverSampler()\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.10, random_state=42)\n",
    "        #tranform train data\n",
    "        freq_term_matrix_train = count_vectorizer.transform(X_train)\n",
    "        tfidf.fit(freq_term_matrix_train)\n",
    "        tf_idf_matrix = tfidf.transform(freq_term_matrix_train)\n",
    "        X_train_resampled ,y_train_resampled = ros.fit_sample(tf_idf_matrix.toarray(), y_train)\n",
    "        print(\"With Over sampling : X_train length=\" + str(len(X_train_resampled)))\n",
    "        #transform test data\n",
    "        freq_term_matrix_test = count_vectorizer.transform(X_test)\n",
    "        tfidf.fit(freq_term_matrix_test)\n",
    "        tf_idf_matrix_test = tfidf.transform(freq_term_matrix_test)\n",
    "        #fit the model with transformed train data\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "        y_pred = model.predict(tf_idf_matrix_test.toarray())\n",
    "        nprecision, pprecision, nrecall, precall, nfscore, pfscore, accuracy = mesaureModelPerformanceSklearn(y_test, y_pred)\n",
    "        totalNPrecision += nprecision\n",
    "        totalPPrecision += pprecision\n",
    "        totalNRecall += nrecall\n",
    "        totalPRecall += precall\n",
    "        totalNFscore += nfscore\n",
    "        totalPFscore += pfscore\n",
    "        totalAccuracy += accuracy\n",
    "    print(dataName + \" data metrics\")\n",
    "    print (modelName + \" classifier avg positive class precision:\" + str((totalPPrecision/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class recall:\" + str((totalPRecall/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class fscore:\" + str((totalPFscore/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class precision:\" + str((totalNPrecision/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class recall:\" + str((totalNRecall/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class fscore:\" + str((totalNFscore/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg accuracy:\" + str((totalAccuracy/10) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def transformAndPredictTFIDF(vocabulary, features, labels, contestant):\n",
    "    performKFoldWithOverSamplingTFIDF(vocabulary, features, labels, \"NB\", MultinomialNB(), contestant)\n",
    "    performKFoldTestingTFIDF(vocabulary,  features, labels, \"NB\", MultinomialNB(), contestant)\n",
    "    performKFoldWithOverSamplingTFIDF(vocabulary, features, labels, \"LinearSVC\", LinearSVC(multi_class='ovr'), contestant)    \n",
    "    performKFoldTestingTFIDF(vocabulary,  features, labels, \"LinearSVC\", LinearSVC(multi_class='ovr'), contestant)\n",
    "#     performKFoldWithOverSamplingTFIDF(vocabulary,  features, labels, \"LogisticRegression\", LogisticRegression(solver='sag'), contestant)\n",
    "#     performKFoldTestingTFIDF(vocabulary,  features, labels, \"LogisticRegression\", LogisticRegression(solver='sag'), contestant)\n",
    "#     performKFoldWithOverSamplingTFIDF(vocabulary,  features, labels, \"DT\", DecisionTreeClassifier(random_state=0), contestant)\n",
    "#     performKFoldTestingTFIDF(vocabulary,  features, labels, \"DT\", DecisionTreeClassifier(random_state=0), contestant)\n",
    "#     performKFoldWithOverSamplingTFIDF(vocabulary,  features, labels, \"KNN\", KNeighborsClassifier(), contestant)\n",
    "#     performKFoldTestingTFIDF(vocabulary,  features, labels, \"KNN\", KNeighborsClassifier(), contestant)\n",
    "#     performKFoldWithOverSamplingTFIDF(vocabulary, features, labels, \"RandomForest\", RandomForestClassifier(n_estimators=50), contestant)    \n",
    "#     performKFoldTestingTFIDF(vocabulary,  features, labels, \"RandomForest\", RandomForestClassifier(n_estimators=50), contestant)\n",
    "#     performKFoldWithOverSamplingTFIDF(vocabulary, features, labels, \"XGBClassifier\", XGBClassifier(), contestant)    \n",
    "#     performKFoldTestingTFIDF(vocabulary,  features, labels, \"XGBClassifier\", XGBClassifier(), contestant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Over sampling : X_train length=5208\n",
      "With Over sampling : X_train length=5208\n",
      "With Over sampling : X_train length=5208\n",
      "With Over sampling : X_train length=5208\n",
      "With Over sampling : X_train length=5208\n",
      "With Over sampling : X_train length=5208\n",
      "With Over sampling : X_train length=5208\n",
      "With Over sampling : X_train length=5208\n",
      "With Over sampling : X_train length=5208\n",
      "With Over sampling : X_train length=5208\n",
      "Obama data metrics\n",
      "NB classifier avg positive class precision:61.4058915813%\n",
      "NB classifier avg positive class recall:71.3068181818%\n",
      "NB classifier avg positive class fscore:65.9847300271%\n",
      "NB classifier avg negative class precision:59.0066153452%\n",
      "NB classifier avg negative class recall:65.1612903226%\n",
      "NB classifier avg negative class fscore:61.9300875981%\n",
      "NB classifier avg accuracy:59.1605839416%\n",
      "Without over sampling X_train length=4923\n",
      "Without over sampling X_train length=4923\n",
      "Without over sampling X_train length=4923\n",
      "Without over sampling X_train length=4923\n",
      "Without over sampling X_train length=4923\n",
      "Without over sampling X_train length=4923\n",
      "Without over sampling X_train length=4923\n",
      "Without over sampling X_train length=4923\n",
      "Without over sampling X_train length=4923\n",
      "Without over sampling X_train length=4923\n",
      "Obama data metrics\n",
      "NB classifier avg positive class precision:65.5629139073%\n",
      "NB classifier avg positive class recall:56.25%\n",
      "NB classifier avg positive class fscore:60.5504587156%\n",
      "NB classifier avg negative class precision:55.9829059829%\n",
      "NB classifier avg negative class recall:70.4301075269%\n",
      "NB classifier avg negative class fscore:62.380952381%\n",
      "NB classifier avg accuracy:57.8467153285%\n",
      "With Over sampling : X_train length=5208\n",
      "With Over sampling : X_train length=5208\n",
      "With Over sampling : X_train length=5208\n",
      "With Over sampling : X_train length=5208\n",
      "With Over sampling : X_train length=5208\n",
      "With Over sampling : X_train length=5208\n",
      "With Over sampling : X_train length=5208\n",
      "With Over sampling : X_train length=5208\n",
      "With Over sampling : X_train length=5208\n",
      "With Over sampling : X_train length=5208\n",
      "Obama data metrics\n",
      "LinearSVC classifier avg positive class precision:61.1102897299%\n",
      "LinearSVC classifier avg positive class recall:60.7954545455%\n",
      "LinearSVC classifier avg positive class fscore:60.9512284886%\n",
      "LinearSVC classifier avg negative class precision:60.1365814492%\n",
      "LinearSVC classifier avg negative class recall:59.3548387097%\n",
      "LinearSVC classifier avg negative class fscore:59.7403417355%\n",
      "LinearSVC classifier avg accuracy:57.4087591241%\n",
      "Without over sampling X_train length=4923\n",
      "Without over sampling X_train length=4923\n",
      "Without over sampling X_train length=4923\n",
      "Without over sampling X_train length=4923\n",
      "Without over sampling X_train length=4923\n",
      "Without over sampling X_train length=4923\n",
      "Without over sampling X_train length=4923\n",
      "Without over sampling X_train length=4923\n",
      "Without over sampling X_train length=4923\n",
      "Without over sampling X_train length=4923\n",
      "Obama data metrics\n",
      "LinearSVC classifier avg positive class precision:62.1301775148%\n",
      "LinearSVC classifier avg positive class recall:59.6590909091%\n",
      "LinearSVC classifier avg positive class fscore:60.8695652174%\n",
      "LinearSVC classifier avg negative class precision:59.8930481283%\n",
      "LinearSVC classifier avg negative class recall:60.2150537634%\n",
      "LinearSVC classifier avg negative class fscore:60.0536193029%\n",
      "LinearSVC classifier avg accuracy:57.4817518248%\n",
      "With Over sampling : X_train length=7827\n",
      "With Over sampling : X_train length=7827\n",
      "With Over sampling : X_train length=7827\n",
      "With Over sampling : X_train length=7827\n",
      "With Over sampling : X_train length=7827\n",
      "With Over sampling : X_train length=7827\n",
      "With Over sampling : X_train length=7827\n",
      "With Over sampling : X_train length=7827\n",
      "With Over sampling : X_train length=7827\n",
      "With Over sampling : X_train length=7827\n",
      "Romney data metrics\n",
      "NB classifier avg positive class precision:47.7817882079%\n",
      "NB classifier avg positive class recall:63.1147540984%\n",
      "NB classifier avg positive class fscore:54.373627123%\n",
      "NB classifier avg negative class precision:69.3652687273%\n",
      "NB classifier avg negative class recall:64.2253521127%\n",
      "NB classifier avg negative class fscore:66.6901467797%\n",
      "NB classifier avg accuracy:57.0442477876%\n",
      "Without over sampling X_train length=5083\n",
      "Without over sampling X_train length=5083\n",
      "Without over sampling X_train length=5083\n",
      "Without over sampling X_train length=5083\n",
      "Without over sampling X_train length=5083\n",
      "Without over sampling X_train length=5083\n",
      "Without over sampling X_train length=5083\n",
      "Without over sampling X_train length=5083\n",
      "Without over sampling X_train length=5083\n",
      "Without over sampling X_train length=5083\n",
      "Romney data metrics\n",
      "NB classifier avg positive class precision:82.3529411765%\n",
      "NB classifier avg positive class recall:11.4754098361%\n",
      "NB classifier avg positive class fscore:20.1438848921%\n",
      "NB classifier avg negative class precision:54.1586073501%\n",
      "NB classifier avg negative class recall:98.5915492958%\n",
      "NB classifier avg negative class fscore:69.9126092385%\n",
      "NB classifier avg accuracy:55.3982300885%\n",
      "With Over sampling : X_train length=7827\n",
      "With Over sampling : X_train length=7827\n",
      "With Over sampling : X_train length=7827\n",
      "With Over sampling : X_train length=7827\n",
      "With Over sampling : X_train length=7827\n",
      "With Over sampling : X_train length=7827\n",
      "With Over sampling : X_train length=7827\n",
      "With Over sampling : X_train length=7827\n",
      "With Over sampling : X_train length=7827\n",
      "With Over sampling : X_train length=7827\n",
      "Romney data metrics\n",
      "LinearSVC classifier avg positive class precision:47.4819188838%\n",
      "LinearSVC classifier avg positive class recall:48.4426229508%\n",
      "LinearSVC classifier avg positive class fscore:47.9451478203%\n",
      "LinearSVC classifier avg negative class precision:67.4935424608%\n",
      "LinearSVC classifier avg negative class recall:65.0%\n",
      "LinearSVC classifier avg negative class fscore:66.2165881206%\n",
      "LinearSVC classifier avg accuracy:54.6725663717%\n",
      "Without over sampling X_train length=5083\n",
      "Without over sampling X_train length=5083\n",
      "Without over sampling X_train length=5083\n",
      "Without over sampling X_train length=5083\n",
      "Without over sampling X_train length=5083\n",
      "Without over sampling X_train length=5083\n",
      "Without over sampling X_train length=5083\n",
      "Without over sampling X_train length=5083\n",
      "Without over sampling X_train length=5083\n",
      "Without over sampling X_train length=5083\n",
      "Romney data metrics\n",
      "LinearSVC classifier avg positive class precision:50.5747126437%\n",
      "LinearSVC classifier avg positive class recall:36.0655737705%\n",
      "LinearSVC classifier avg positive class fscore:42.1052631579%\n",
      "LinearSVC classifier avg negative class precision:64.2424242424%\n",
      "LinearSVC classifier avg negative class recall:74.6478873239%\n",
      "LinearSVC classifier avg negative class fscore:69.0553745928%\n",
      "LinearSVC classifier avg accuracy:56.2831858407%\n"
     ]
    }
   ],
   "source": [
    "transformAndPredictTFIDF(vocabObama, featuresObama, labelsObama, \"Obama\")\n",
    "transformAndPredictTFIDF(vocabRomney, featuresRomney, labelsRomney, \"Romney\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def performKFoldTestingTFIDF_Test(vocabulary, features, labels,vocabulary_test, features_test, labels_test ,modelName, model, dataName):\n",
    "    totalNPrecision = 0\n",
    "    totalPPrecision = 0\n",
    "    totalNRecall = 0\n",
    "    totalPRecall = 0\n",
    "    totalNFscore = 0\n",
    "    totalPFscore = 0\n",
    "    totalAccuracy = 0\n",
    "    y = np.array(labels)\n",
    "\n",
    "    X_train = features\n",
    "    y_train = y\n",
    "    X_test = features_test\n",
    "    y_test = np.array(labels_test)\n",
    "    \n",
    "    count_vectorizer = CountVectorizer(min_df=1.0,vocabulary=set(vocabulary),ngram_range=(1, 3))\n",
    "    tfidf = TfidfTransformer(norm=\"l2\")\n",
    "    freq_term_matrix = count_vectorizer.fit_transform(X_train)\n",
    "    tf_idf_matrix = tfidf.fit_transform(freq_term_matrix)\n",
    "    \n",
    "    freq_term_matrix_test = count_vectorizer.transform(X_test)\n",
    "    tf_idf_matrix_test = tfidf.transform(freq_term_matrix_test)\n",
    "    \n",
    "    model.fit(tf_idf_matrix.toarray(), y_train)\n",
    "\n",
    "    \n",
    "    y_pred = model.predict(tf_idf_matrix_test.toarray())\n",
    "    nprecision, pprecision, nrecall, precall, nfscore, pfscore, accuracy = mesaureModelPerformanceSklearn(y_test, y_pred)\n",
    "    totalNPrecision += nprecision\n",
    "    totalPPrecision += pprecision\n",
    "    totalNRecall += nrecall\n",
    "    totalPRecall += precall\n",
    "    totalNFscore += nfscore\n",
    "    totalPFscore += pfscore\n",
    "    totalAccuracy += accuracy\n",
    "\n",
    "    print(dataName + \"Test data metrics without oversampling\")\n",
    "    print (modelName + \" classifier avg positive class precision:\" + str(totalPPrecision * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class recall:\" + str(totalPRecall * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class fscore:\" + str(totalPFscore * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class precision:\" + str(totalNPrecision * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class recall:\" + str(totalNRecall * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class fscore:\" + str(totalNFscore * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg accuracy:\" + str(totalAccuracy * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "def performKFoldWithOverSamplingTFIDF_Test(vocabulary, features, labels,vocabulary_test, features_test, labels_test ,modelName, model, dataName):\n",
    "    totalNPrecision = 0\n",
    "    totalPPrecision = 0\n",
    "    totalNRecall = 0\n",
    "    totalPRecall = 0\n",
    "    totalNFscore = 0\n",
    "    totalPFscore = 0\n",
    "    totalAccuracy = 0\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    X_train = features\n",
    "    y_train = y\n",
    "    X_test = features_test\n",
    "    y_test = np.array(labels_test)\n",
    "    \n",
    "    count_vectorizer = CountVectorizer(min_df=1, vocabulary=set(vocabulary), ngram_range=(1, 3))\n",
    "    freq_term_matrix = count_vectorizer.fit_transform(X_train)\n",
    "    tfidf = TfidfTransformer(norm=\"l2\")\n",
    "    tf_idf_matrix = tfidf.fit_transform(freq_term_matrix)\n",
    "    \n",
    "    ros = RandomOverSampler()\n",
    "    \n",
    "    X_train_resampled ,y_train_resampled = ros.fit_sample(tf_idf_matrix.toarray(), y_train)\n",
    "    \n",
    "    freq_term_matrix_test = count_vectorizer.transform(X_test)\n",
    "    tf_idf_matrix_test = tfidf.transform(freq_term_matrix_test)\n",
    "    \n",
    "    #fit the model with transformed train data\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    y_pred = model.predict(tf_idf_matrix_test.toarray())\n",
    "    nprecision, pprecision, nrecall, precall, nfscore, pfscore, accuracy = mesaureModelPerformanceSklearn(y_test, y_pred)\n",
    "    totalNPrecision += nprecision\n",
    "    totalPPrecision += pprecision\n",
    "    totalNRecall += nrecall\n",
    "    totalPRecall += precall\n",
    "    totalNFscore += nfscore\n",
    "    totalPFscore += pfscore\n",
    "    totalAccuracy += accuracy\n",
    "    print(dataName + \" Test data metrics with oversampling\")\n",
    "    print (modelName + \" classifier avg positive class precision:\" + str(totalPPrecision * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class recall:\" + str(totalPRecall * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class fscore:\" + str(totalPFscore * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class precision:\" + str(totalNPrecision * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class recall:\" + str(totalNRecall * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class fscore:\" + str(totalNFscore * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg accuracy:\" + str(totalAccuracy * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def transformAndPredictTFIDF_Test(vocabulary, features, labels,vocabulary_test, features_test, labels_test, contestant):\n",
    "    performKFoldTestingTFIDF_Test(vocabulary,  features, labels, vocabulary_test, features_test, labels_test, \"NB\", MultinomialNB(), contestant)\n",
    "    performKFoldWithOverSamplingTFIDF_Test(vocabulary,  features, labels, vocabulary_test, features_test, labels_test, \"NB\", MultinomialNB(), contestant)\n",
    "    performKFoldTestingTFIDF_Test(vocabulary,  features, labels, vocabulary_test, features_test, labels_test, \"LinearSVC\", LinearSVC(C=1.8, multi_class='ovr'), contestant)\n",
    "    performKFoldWithOverSamplingTFIDF_Test(vocabulary,  features, labels, vocabulary_test, features_test, labels_test, \"LinearSVC\", LinearSVC(C=1.8, multi_class='ovr'), contestant)\n",
    "    performKFoldTestingTFIDF_Test(vocabulary,  features, labels, vocabulary_test, features_test, labels_test, \"LogisticRegression\", LogisticRegression(solver='sag'), contestant)\n",
    "    performKFoldWithOverSamplingTFIDF_Test(vocabulary,  features, labels, vocabulary_test, features_test, labels_test, \"LogisticRegression\", LogisticRegression(solver='sag'), contestant)\n",
    "    #performKFoldWithOverSamplingTFIDF(vocabulary, features, labels, \"LinearSVC\", LinearSVC(multi_class='ovr'), contestant)    \n",
    "    #performKFoldTestingTFIDF(vocabulary,  features, labels, \"LinearSVC\", LinearSVC(multi_class='ovr'), contestant)\n",
    "#     performKFoldWithOverSamplingTFIDF(vocabulary,  features, labels, \"LogisticRegression\", LogisticRegression(solver='sag'), contestant)\n",
    "#     performKFoldTestingTFIDF(vocabulary,  features, labels, \"LogisticRegression\", LogisticRegression(solver='sag'), contestant)\n",
    "#     performKFoldWithOverSamplingTFIDF(vocabulary,  features, labels, \"DT\", DecisionTreeClassifier(random_state=0), contestant)\n",
    "#     performKFoldTestingTFIDF(vocabulary,  features, labels, \"DT\", DecisionTreeClassifier(random_state=0), contestant)\n",
    "#     performKFoldWithOverSamplingTFIDF(vocabulary,  features, labels, \"KNN\", KNeighborsClassifier(), contestant)\n",
    "#     performKFoldTestingTFIDF(vocabulary,  features, labels, \"KNN\", KNeighborsClassifier(), contestant)\n",
    "#     performKFoldWithOverSamplingTFIDF(vocabulary, features, labels, \"RandomForest\", RandomForestClassifier(n_estimators=50), contestant)    \n",
    "#     performKFoldTestingTFIDF(vocabulary,  features, labels, \"RandomForest\", RandomForestClassifier(n_estimators=50), contestant)\n",
    "#     performKFoldWithOverSamplingTFIDF(vocabulary, features, labels, \"XGBClassifier\", XGBClassifier(), contestant)    \n",
    "#     performKFoldTestingTFIDF(vocabulary,  features, labels, \"XGBClassifier\", XGBClassifier(), contestant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObamaTest data metrics without oversampling\n",
      "NB classifier avg positive class precision:59.2592592593%\n",
      "NB classifier avg positive class recall:46.735395189%\n",
      "NB classifier avg positive class fscore:52.2574447646%\n",
      "NB classifier avg negative class precision:53.2275132275%\n",
      "NB classifier avg negative class recall:73.1104651163%\n",
      "NB classifier avg negative class fscore:61.6044090631%\n",
      "NB classifier avg accuracy:55.4074833419%\n",
      "Obama Test data metrics with oversampling\n",
      "NB classifier avg positive class precision:52.5502318393%\n",
      "NB classifier avg positive class recall:58.4192439863%\n",
      "NB classifier avg positive class fscore:55.3295362083%\n",
      "NB classifier avg negative class precision:55.2727272727%\n",
      "NB classifier avg negative class recall:66.2790697674%\n",
      "NB classifier avg negative class fscore:60.2775941837%\n",
      "NB classifier avg accuracy:54.4848795489%\n",
      "ObamaTest data metrics without oversampling\n",
      "LinearSVC classifier avg positive class precision:56.561922366%\n",
      "LinearSVC classifier avg positive class recall:52.5773195876%\n",
      "LinearSVC classifier avg positive class fscore:54.4968833482%\n",
      "LinearSVC classifier avg negative class precision:57.0432357043%\n",
      "LinearSVC classifier avg negative class recall:59.4476744186%\n",
      "LinearSVC classifier avg negative class fscore:58.2206405694%\n",
      "LinearSVC classifier avg accuracy:54.6386468478%\n",
      "Obama Test data metrics with oversampling\n",
      "LinearSVC classifier avg positive class precision:54.1592920354%\n",
      "LinearSVC classifier avg positive class recall:52.5773195876%\n",
      "LinearSVC classifier avg positive class fscore:53.3565823888%\n",
      "LinearSVC classifier avg negative class precision:57.1022727273%\n",
      "LinearSVC classifier avg negative class recall:58.4302325581%\n",
      "LinearSVC classifier avg negative class fscore:57.7586206897%\n",
      "LinearSVC classifier avg accuracy:53.6647872886%\n",
      "ObamaTest data metrics without oversampling\n",
      "LogisticRegression classifier avg positive class precision:59.266802444%\n",
      "LogisticRegression classifier avg positive class recall:50.0%\n",
      "LogisticRegression classifier avg positive class fscore:54.2404473439%\n",
      "LogisticRegression classifier avg negative class precision:56.2176165803%\n",
      "LogisticRegression classifier avg negative class recall:63.0813953488%\n",
      "LogisticRegression classifier avg negative class fscore:59.4520547945%\n",
      "LogisticRegression classifier avg accuracy:55.7662737058%\n",
      "Obama Test data metrics with oversampling\n",
      "LogisticRegression classifier avg positive class precision:56.7811934901%\n",
      "LogisticRegression classifier avg positive class recall:53.9518900344%\n",
      "LogisticRegression classifier avg positive class fscore:55.3303964758%\n",
      "LogisticRegression classifier avg negative class precision:57.1041948579%\n",
      "LogisticRegression classifier avg negative class recall:61.3372093023%\n",
      "LogisticRegression classifier avg negative class fscore:59.1450595655%\n",
      "LogisticRegression classifier avg accuracy:55.8175294721%\n",
      "RomneyTest data metrics without oversampling\n",
      "NB classifier avg positive class precision:68.3333333333%\n",
      "NB classifier avg positive class recall:10.6493506494%\n",
      "NB classifier avg positive class fscore:18.4269662921%\n",
      "NB classifier avg negative class precision:54.439930354%\n",
      "NB classifier avg negative class recall:97.7083333333%\n",
      "NB classifier avg negative class fscore:69.9217294074%\n",
      "NB classifier avg accuracy:55.4210526316%\n",
      "Romney Test data metrics with oversampling\n",
      "NB classifier avg positive class precision:48.2546201232%\n",
      "NB classifier avg positive class recall:61.038961039%\n",
      "NB classifier avg positive class fscore:53.8990825688%\n",
      "NB classifier avg negative class precision:69.1065662002%\n",
      "NB classifier avg negative class recall:66.875%\n",
      "NB classifier avg negative class fscore:67.9724722075%\n",
      "NB classifier avg accuracy:58.2105263158%\n",
      "RomneyTest data metrics without oversampling\n",
      "LinearSVC classifier avg positive class precision:54.4117647059%\n",
      "LinearSVC classifier avg positive class recall:48.0519480519%\n",
      "LinearSVC classifier avg positive class fscore:51.0344827586%\n",
      "LinearSVC classifier avg negative class precision:64.3611911623%\n",
      "LinearSVC classifier avg negative class recall:69.7916666667%\n",
      "LinearSVC classifier avg negative class fscore:66.9665167416%\n",
      "LinearSVC classifier avg accuracy:56.5263157895%\n",
      "Romney Test data metrics with oversampling\n",
      "LinearSVC classifier avg positive class precision:47.9012345679%\n",
      "LinearSVC classifier avg positive class recall:50.3896103896%\n",
      "LinearSVC classifier avg positive class fscore:49.1139240506%\n",
      "LinearSVC classifier avg negative class precision:66.4495114007%\n",
      "LinearSVC classifier avg negative class recall:63.75%\n",
      "LinearSVC classifier avg negative class fscore:65.0717703349%\n",
      "LinearSVC classifier avg accuracy:55.2105263158%\n",
      "RomneyTest data metrics without oversampling\n",
      "LogisticRegression classifier avg positive class precision:71.8918918919%\n",
      "LogisticRegression classifier avg positive class recall:34.5454545455%\n",
      "LogisticRegression classifier avg positive class fscore:46.6666666667%\n",
      "LogisticRegression classifier avg negative class precision:60.4093154552%\n",
      "LogisticRegression classifier avg negative class recall:89.1666666667%\n",
      "LogisticRegression classifier avg negative class fscore:72.0235591081%\n",
      "LogisticRegression classifier avg accuracy:60.1578947368%\n",
      "Romney Test data metrics with oversampling\n",
      "LogisticRegression classifier avg positive class precision:49.4692144374%\n",
      "LogisticRegression classifier avg positive class recall:60.5194805195%\n",
      "LogisticRegression classifier avg positive class fscore:54.4392523364%\n",
      "LogisticRegression classifier avg negative class precision:69.7881828317%\n",
      "LogisticRegression classifier avg negative class recall:65.2083333333%\n",
      "LogisticRegression classifier avg negative class fscore:67.4205708131%\n",
      "LogisticRegression classifier avg accuracy:58.4210526316%\n"
     ]
    }
   ],
   "source": [
    "transformAndPredictTFIDF_Test(vocabObama, featuresObama, labelsObama,vocabObamaTest, featuresObamaTest, labelsObamaTest, \"Obama\")\n",
    "transformAndPredictTFIDF_Test(vocabRomney, featuresRomney, labelsRomney, vocabRomneyTest, featuresRomneyTest, labelsRomneyTest,\"Romney\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def performKFoldWithCountVectorizer(features, labels, modelName, model, dataName):\n",
    "    totalNPrecision = 0\n",
    "    totalPPrecision = 0\n",
    "    totalNRecall = 0\n",
    "    totalPRecall = 0\n",
    "    totalNFscore = 0\n",
    "    totalPFscore = 0\n",
    "    totalAccuracy = 0\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    kf.get_n_splits(features)\n",
    "    #X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        nprecision, pprecision, nrecall, precall, nfscore, pfscore, accuracy = mesaureModelPerformanceSklearn(y_test, y_pred)\n",
    "        totalNPrecision += nprecision\n",
    "        totalPPrecision += pprecision\n",
    "        totalNRecall += nrecall\n",
    "        totalPRecall += precall\n",
    "        totalNFscore += nfscore\n",
    "        totalPFscore += pfscore\n",
    "        totalAccuracy += accuracy\n",
    "    print(dataName + \" data metrics without oversampling\")\n",
    "    print (modelName + \" classifier avg positive class precision:\" + str((totalPPrecision/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class recall:\" + str((totalPRecall/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class fscore:\" + str((totalPFscore/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class precision:\" + str((totalNPrecision/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class recall:\" + str((totalNRecall/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class fscore:\" + str((totalNFscore/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg accuracy:\" + str((totalAccuracy/10) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def performKFoldWithOverSamplingWithCountVectorizer(features, labels, modelName, model, dataName):\n",
    "    totalNPrecision = 0\n",
    "    totalPPrecision = 0\n",
    "    totalNRecall = 0\n",
    "    totalPRecall = 0\n",
    "    totalNFscore = 0\n",
    "    totalPFscore = 0\n",
    "    totalAccuracy = 0\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    \n",
    "    # Apply the random over-sampling\n",
    "    ros = RandomOverSampler()\n",
    "    kf.get_n_splits(features)\n",
    "    \n",
    "    y = np.array(labels)\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        X_train, X_test = features[train_index], features[test_index] \n",
    "        print(\"TRAIN:\", len(X_train), \"TEST:\", len(X_test))\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        X_resampled ,y_resampled = ros.fit_sample(X_train, y_train)\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        y_pred = model.predict(X_test)\n",
    "        #print(y_pred)\n",
    "        nprecision, pprecision, nrecall, precall, nfscore, pfscore, accuracy = mesaureModelPerformanceSklearn(y_test, y_pred)\n",
    "        totalNPrecision += nprecision\n",
    "        totalPPrecision += pprecision\n",
    "        totalNRecall += nrecall\n",
    "        totalPRecall += precall\n",
    "        totalNFscore += nfscore\n",
    "        totalPFscore += pfscore\n",
    "        totalAccuracy += accuracy\n",
    "    print(dataName + \" data metrics with oversampling\")\n",
    "    print (modelName + \" classifier avg positive class precision:\" + str((totalPPrecision/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class recall:\" + str((totalPRecall/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class fscore:\" + str((totalPFscore/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class precision:\" + str((totalNPrecision/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class recall:\" + str((totalNRecall/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class fscore:\" + str((totalNFscore/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg accuracy:\" + str((totalAccuracy/10) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def transformAndPredictWithCountVectorizer(vocabulary, features, labels, contestant):\n",
    "    count_vectorizer = CountVectorizer(min_df=1.0, vocabulary=set(vocabulary), ngram_range=(1, 3))\n",
    "    count_vectorizer.fit(features);\n",
    "    freq_term_matrix = count_vectorizer.transform(features)\n",
    "#     performKFoldWithCountVectorizer(freq_term_matrix.toarray(), labels, \"NB\", MultinomialNB(), contestant)\n",
    "#     performKFoldWithCountVectorizer(freq_term_matrix.toarray(), labels, \"LinearSVC\", LinearSVC(multi_class='ovr', max_iter=1000), contestant)\n",
    "#     performKFoldWithCountVectorizer(freq_term_matrix.toarray(), labels, \"DT\", DecisionTreeClassifier(random_state=0), contestant)\n",
    "#     performKFoldWithCountVectorizer(freq_term_matrix.toarray(), labels, \"KNN\", KNeighborsClassifier(), contestant)\n",
    "#     performKFoldWithCountVectorizer(freq_term_matrix.toarray(), labels, \"LogisticRegression\", LogisticRegression(solver='sag'), contestant)\n",
    "    performKFoldWithCountVectorizer(freq_term_matrix.toarray(), labels, \"Randomforest\", RandomForestClassifier(n_estimators=50), contestant)\n",
    "    performKFoldWithCountVectorizer(freq_term_matrix.toarray(), labels, \"XGBClassifier\", XGBClassifier(), contestant)\n",
    "#     performKFoldWithOverSamplingWithCountVectorizer(freq_term_matrix.toarray(), labels, \"NB\", MultinomialNB(), contestant)\n",
    "#     performKFoldWithOverSamplingWithCountVectorizer(freq_term_matrix.toarray(), labels, \"LinearSVC\", LinearSVC(multi_class='ovr', max_iter=1000), contestant)\n",
    "#     performKFoldWithOverSamplingWithCountVectorizer(freq_term_matrix.toarray(), labels, \"DT\", DecisionTreeClassifier(random_state=0), contestant)\n",
    "#     performKFoldWithOverSamplingWithCountVectorizer(freq_term_matrix.toarray(), labels, \"KNN\", KNeighborsClassifier(), contestant)\n",
    "#     performKFoldWithOverSamplingWithCountVectorizer(freq_term_matrix.toarray(), labels, \"LogisticRegression\", LogisticRegression(solver='sag'), contestant)\n",
    "    performKFoldWithOverSamplingWithCountVectorizer(freq_term_matrix.toarray(), labels, \"Randomforest\", RandomForestClassifier(n_estimators=50), contestant)\n",
    "    performKFoldWithOverSamplingWithCountVectorizer(freq_term_matrix.toarray(), labels, \"XGBClassifier\", XGBClassifier(), contestant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformAndPredictWithCountVectorizer(vocabObama, featuresObama, labelsObama, \"Obama\")\n",
    "transformAndPredictWithCountVectorizer(vocabRomney, featuresRomney, labelsRomney, \"Romney\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def performKFoldWithCountVectorizer_Test(features, labels, features_test, labels_test, modelName, model, dataName):\n",
    "    totalNPrecision = 0\n",
    "    totalPPrecision = 0\n",
    "    totalNRecall = 0\n",
    "    totalPRecall = 0\n",
    "    totalNFscore = 0\n",
    "    totalPFscore = 0\n",
    "    totalAccuracy = 0\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    kf.get_n_splits(features)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    X_train = features\n",
    "    y_train = y\n",
    "    \n",
    "    X_test = features_test\n",
    "    y_test = np.array(labels_test) \n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    nprecision, pprecision, nrecall, precall, nfscore, pfscore, accuracy = mesaureModelPerformanceSklearn(y_test, y_pred)\n",
    "    totalNPrecision += nprecision\n",
    "    totalPPrecision += pprecision\n",
    "    totalNRecall += nrecall\n",
    "    totalPRecall += precall\n",
    "    totalNFscore += nfscore\n",
    "    totalPFscore += pfscore\n",
    "    totalAccuracy += accuracy\n",
    "    \n",
    "    print(dataName + \" Test data metrics without oversampling\")\n",
    "    print (modelName + \" classifier avg positive class precision:\" + str(totalPPrecision * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class recall:\" + str(totalPRecall* 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class fscore:\" + str(totalPFscore * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class precision:\" + str(totalNPrecision * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class recall:\" + str(totalNRecall * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class fscore:\" + str(totalNFscore * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg accuracy:\" + str(totalAccuracy * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def performKFoldWithOverSamplingWithCountVectorizer_Test(features, labels,features_test, labels_test, modelName, model, dataName):\n",
    "    totalNPrecision = 0\n",
    "    totalPPrecision = 0\n",
    "    totalNRecall = 0\n",
    "    totalPRecall = 0\n",
    "    totalNFscore = 0\n",
    "    totalPFscore = 0\n",
    "    totalAccuracy = 0\n",
    "       \n",
    "    # Apply the random over-sampling\n",
    "    ros = RandomOverSampler()\n",
    "    X_train = features\n",
    "    X_test = features_test\n",
    "    y_train =  np.array(labels)\n",
    "    y_test = np.array(labels_test)\n",
    "    \n",
    "    X_resampled ,y_resampled = ros.fit_sample(X_train, y_train)\n",
    "    model.fit(X_resampled, y_resampled)\n",
    "    y_pred = model.predict(X_test)\n",
    "    nprecision, pprecision, nrecall, precall, nfscore, pfscore, accuracy = mesaureModelPerformanceSklearn(y_test, y_pred)\n",
    "    totalNPrecision += nprecision\n",
    "    totalPPrecision += pprecision\n",
    "    totalNRecall += nrecall\n",
    "    totalPRecall += precall\n",
    "    totalNFscore += nfscore\n",
    "    totalPFscore += pfscore\n",
    "    totalAccuracy += accuracy\n",
    "    print(dataName + \" Test data metrics with oversampling\")\n",
    "    print (modelName + \" classifier avg positive class precision:\" + str(totalPPrecision * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class recall:\" + str(totalPRecall * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class fscore:\" + str(totalPFscore * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class precision:\" + str(totalNPrecision * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class recall:\" + str(totalNRecall * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class fscore:\" + str(totalNFscore * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg accuracy:\" + str(totalAccuracy * 100) + \"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def transformAndPredictWithCountVectorizer_Test(vocabulary, features, labels,vocabulary_test, features_test, labels_test, contestant):\n",
    "    count_vectorizer = CountVectorizer(min_df=1.0, vocabulary=set(vocabulary), ngram_range=(1, 3))\n",
    "    count_vectorizer.fit(features);\n",
    "    \n",
    "    freq_term_matrix = count_vectorizer.transform(features)\n",
    "    freq_term_matrix_test = count_vectorizer.transform(features_test)\n",
    "    \n",
    "    performKFoldWithCountVectorizer_Test(freq_term_matrix.toarray(), labels, freq_term_matrix_test.toarray(),labels_test,\"NB\", MultinomialNB(), contestant)\n",
    "    performKFoldWithOverSamplingWithCountVectorizer_Test(freq_term_matrix.toarray(), labels, freq_term_matrix_test.toarray(),labels_test,\"NB\", MultinomialNB(), contestant)\n",
    "    performKFoldWithCountVectorizer_Test(freq_term_matrix.toarray(), labels, freq_term_matrix_test.toarray(),labels_test,\"LinearSVC\", LinearSVC(multi_class='ovr', max_iter=1000), contestant)\n",
    "    performKFoldWithOverSamplingWithCountVectorizer_Test(freq_term_matrix.toarray(), labels, freq_term_matrix_test.toarray(),labels_test,\"LinearSVC\", LinearSVC(multi_class='ovr', max_iter=1000), contestant)\n",
    "#     performKFoldWithCountVectorizer(freq_term_matrix.toarray(), labels, \"NB\", MultinomialNB(), contestant)\n",
    "#     performKFoldWithCountVectorizer(freq_term_matrix.toarray(), labels, \"LinearSVC\", LinearSVC(multi_class='ovr', max_iter=1000), contestant)\n",
    "#     performKFoldWithCountVectorizer(freq_term_matrix.toarray(), labels, \"DT\", DecisionTreeClassifier(random_state=0), contestant)\n",
    "#     performKFoldWithCountVectorizer(freq_term_matrix.toarray(), labels, \"KNN\", KNeighborsClassifier(), contestant)\n",
    "#     performKFoldWithCountVectorizer(freq_term_matrix.toarray(), labels, \"LogisticRegression\", LogisticRegression(solver='sag'), contestant)\n",
    "    #performKFoldWithCountVectorizer(freq_term_matrix.toarray(), labels, \"Randomforest\", RandomForestClassifier(n_estimators=50), contestant)\n",
    "    #performKFoldWithCountVectorizer(freq_term_matrix.toarray(), labels, \"XGBClassifier\", XGBClassifier(), contestant)\n",
    "#     performKFoldWithOverSamplingWithCountVectorizer(freq_term_matrix.toarray(), labels, \"NB\", MultinomialNB(), contestant)\n",
    "#     performKFoldWithOverSamplingWithCountVectorizer(freq_term_matrix.toarray(), labels, \"LinearSVC\", LinearSVC(multi_class='ovr', max_iter=1000), contestant)\n",
    "#     performKFoldWithOverSamplingWithCountVectorizer(freq_term_matrix.toarray(), labels, \"DT\", DecisionTreeClassifier(random_state=0), contestant)\n",
    "#     performKFoldWithOverSamplingWithCountVectorizer(freq_term_matrix.toarray(), labels, \"KNN\", KNeighborsClassifier(), contestant)\n",
    "#     performKFoldWithOverSamplingWithCountVectorizer(freq_term_matrix.toarray(), labels, \"LogisticRegression\", LogisticRegression(solver='sag'), contestant)\n",
    "    #performKFoldWithOverSamplingWithCountVectorizer(freq_term_matrix.toarray(), labels, \"Randomforest\", RandomForestClassifier(n_estimators=50), contestant)\n",
    "    #performKFoldWithOverSamplingWithCountVectorizer(freq_term_matrix.toarray(), labels, \"XGBClassifier\", XGBClassifier(), contestant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformAndPredictWithCountVectorizer_Test(vocabObama, featuresObama, labelsObama,vocabObamaTest,featuresObamaTest,labelsObamaTest, \"Obama\")\n",
    "transformAndPredictWithCountVectorizer_Test(vocabRomney, featuresRomney, labelsRomney, vocabRomneyTest,featuresRomneyTest,labelsRomneyTest, \"Romney\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def performKFoldTempTestingHashVectorizer(features, labels, modelName, model, dataName):\n",
    "    totalNPrecision = 0\n",
    "    totalPPrecision = 0\n",
    "    totalNRecall = 0\n",
    "    totalPRecall = 0\n",
    "    totalNFscore = 0\n",
    "    totalPFscore = 0\n",
    "    totalAccuracy = 0\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    kf.get_n_splits(features)\n",
    "    #X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        nprecision, pprecision, nrecall, precall, nfscore, pfscore, accuracy = mesaureModelPerformanceSklearn(y_test, y_pred)\n",
    "        totalNPrecision += nprecision\n",
    "        totalPPrecision += pprecision\n",
    "        totalNRecall += nrecall\n",
    "        totalPRecall += precall\n",
    "        totalNFscore += nfscore\n",
    "        totalPFscore += pfscore\n",
    "        totalAccuracy += accuracy\n",
    "    print(dataName + \" data metrics\")\n",
    "    print (modelName + \" classifier avg positive class precision:\" + str((totalPPrecision/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class recall:\" + str((totalPRecall/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class fscore:\" + str((totalPFscore/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class precision:\" + str((totalNPrecision/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class recall:\" + str((totalNRecall/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class fscore:\" + str((totalNFscore/10) * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg accuracy:\" + str((totalAccuracy/10) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "def transformAndPredictHashVectorizer(vocabulary, features, labels, contestant):\n",
    "    hash_vectorizer = HashingVectorizer(ngram_range=(1, 3))\n",
    "    hash_matrix = hash_vectorizer.fit_transform(features)    \n",
    "#     performKFoldTempTestingHashVectorizer(hash_matrix, labels, \"LinearSVC\", LinearSVC(multi_class='ovr', max_iter=1000), contestant)\n",
    "#     performKFoldTempTestingHashVectorizer(hash_matrix, labels, \"DT\", DecisionTreeClassifier(random_state=0), contestant)\n",
    "#     performKFoldTempTestingHashVectorizer(hash_matrix, labels, \"KNN\", KNeighborsClassifier(), contestant)\n",
    "    performKFoldTempTestingHashVectorizer(hash_matrix, labels, \"RandomForest\", RandomForestClassifier(n_estimators=50), contestant)\n",
    "#     performKFoldTempTestingHashVectorizer(hash_matrix, labels, \"XGBClassifier\", XGBClassifier(), contestant)\n",
    "#     performKFoldTempTestingHashVectorizer(hash_matrix, labels, \"LogisticRegression\", LogisticRegression(), contestant)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformAndPredictHashVectorizer(vocabObama, featuresObama, labelsObama, \"Obama\")\n",
    "transformAndPredictHashVectorizer(vocabRomney, featuresRomney, labelsRomney, \"Romney\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def performKFoldTempTestingHashVectorizer_Test(features, labels,features_test, labels_test, modelName, model, dataName):\n",
    "    totalNPrecision = 0\n",
    "    totalPPrecision = 0\n",
    "    totalNRecall = 0\n",
    "    totalPRecall = 0\n",
    "    totalNFscore = 0\n",
    "    totalPFscore = 0\n",
    "    totalAccuracy = 0\n",
    "    X_train = features\n",
    "    X_test = features_test\n",
    "    y_train = np.array(labels)\n",
    "    y_test = np.array(labels_test)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    nprecision, pprecision, nrecall, precall, nfscore, pfscore, accuracy = mesaureModelPerformanceSklearn(y_test, y_pred)\n",
    "    totalNPrecision += nprecision\n",
    "    totalPPrecision += pprecision\n",
    "    totalNRecall += nrecall\n",
    "    totalPRecall += precall\n",
    "    totalNFscore += nfscore\n",
    "    totalPFscore += pfscore\n",
    "    totalAccuracy += accuracy\n",
    "    print(dataName + \" test data metrics\")\n",
    "    print (modelName + \" classifier avg positive class precision:\" + str(totalPPrecision * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class recall:\" + str(totalPRecall * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg positive class fscore:\" + str(totalPFscore * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class precision:\" + str(totalNPrecision * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class recall:\" + str(totalNRecall * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg negative class fscore:\" + str(totalNFscore * 100) + \"%\")\n",
    "    print (modelName + \" classifier avg accuracy:\" + str(totalAccuracy * 100) + \"%\")\n",
    "    \n",
    "    \n",
    "#     for train_index, test_index in kf.split(features):\n",
    "#         print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "#         X_train, X_test = features[train_index], features[test_index]\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "#         model.fit(X_train, y_train)\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         nprecision, pprecision, nrecall, precall, nfscore, pfscore, accuracy = mesaureModelPerformanceSklearn(y_test, y_pred)\n",
    "#         totalNPrecision += nprecision\n",
    "#         totalPPrecision += pprecision\n",
    "#         totalNRecall += nrecall\n",
    "#         totalPRecall += precall\n",
    "#         totalNFscore += nfscore\n",
    "#         totalPFscore += pfscore\n",
    "#         totalAccuracy += accuracy\n",
    "#     print(dataName + \" test data metrics\")\n",
    "#     print (modelName + \" classifier avg positive class precision:\" + str((totalPPrecision/10) * 100) + \"%\")\n",
    "#     print (modelName + \" classifier avg positive class recall:\" + str((totalPRecall/10) * 100) + \"%\")\n",
    "#     print (modelName + \" classifier avg positive class fscore:\" + str((totalPFscore/10) * 100) + \"%\")\n",
    "#     print (modelName + \" classifier avg negative class precision:\" + str((totalNPrecision/10) * 100) + \"%\")\n",
    "#     print (modelName + \" classifier avg negative class recall:\" + str((totalNRecall/10) * 100) + \"%\")\n",
    "#     print (modelName + \" classifier avg negative class fscore:\" + str((totalNFscore/10) * 100) + \"%\")\n",
    "#     print (modelName + \" classifier avg accuracy:\" + str((totalAccuracy/10) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "def transformAndPredictHashVectorizer_Test(vocabulary, features, labels, vocabulary_test, features_test, labels_test, contestant):\n",
    "    hash_vectorizer = HashingVectorizer(ngram_range=(1, 4))\n",
    "    hash_matrix = hash_vectorizer.fit_transform(features)   \n",
    "    hash_matrix_test = hash_vectorizer.transform(features_test)\n",
    "    performKFoldTempTestingHashVectorizer_Test(hash_matrix, labels,hash_matrix_test, labels_test, \"LinearSVC\", LinearSVC(multi_class='ovr', max_iter=1000), contestant)\n",
    "#     performKFoldTempTestingHashVectorizer(hash_matrix, labels, \"DT\", DecisionTreeClassifier(random_state=0), contestant)\n",
    "#     performKFoldTempTestingHashVectorizer(hash_matrix, labels, \"KNN\", KNeighborsClassifier(), contestant)\n",
    "#     performKFoldTempTestingHashVectorizer_Test(hash_matrix, labels, hash_matrix_test, labels_test, \"RandomForest\", RandomForestClassifier(n_estimators=50), contestant)\n",
    "#     performKFoldTempTestingHashVectorizer(hash_matrix, labels, \"XGBClassifier\", XGBClassifier(), contestant)\n",
    "#     performKFoldTempTestingHashVectorizer(hash_matrix, labels, \"LogisticRegression\", LogisticRegression(), contestant)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama test data metrics\n",
      "LinearSVC classifier avg positive class precision:61.2576064909%\n",
      "LinearSVC classifier avg positive class recall:51.8900343643%\n",
      "LinearSVC classifier avg positive class fscore:56.1860465116%\n",
      "LinearSVC classifier avg negative class precision:57.463672391%\n",
      "LinearSVC classifier avg negative class recall:63.226744186%\n",
      "LinearSVC classifier avg negative class fscore:60.2076124567%\n",
      "LinearSVC classifier avg accuracy:57.0989236289%\n",
      "Romney test data metrics\n",
      "LinearSVC classifier avg positive class precision:69.7872340426%\n",
      "LinearSVC classifier avg positive class recall:42.5974025974%\n",
      "LinearSVC classifier avg positive class fscore:52.9032258065%\n",
      "LinearSVC classifier avg negative class precision:62.4626865672%\n",
      "LinearSVC classifier avg negative class recall:87.1875%\n",
      "LinearSVC classifier avg negative class fscore:72.7826086957%\n",
      "LinearSVC classifier avg accuracy:62.1578947368%\n"
     ]
    }
   ],
   "source": [
    "transformAndPredictHashVectorizer_Test(vocabObama, featuresObama, labelsObama,vocabObamaTest,featuresObamaTest,labelsObamaTest, \"Obama\")\n",
    "transformAndPredictHashVectorizer_Test(vocabRomney, featuresRomney, labelsRomney, vocabRomneyTest,featuresRomneyTest,labelsRomneyTest, \"Romney\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
